{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Predicting Loan Risk using SparkML on IBM Cloud Pak for Data as a Service"}, {"metadata": {}, "cell_type": "markdown", "source": "We'll use this notebook to create a machine learning model to predict customer churn. In this notebook we will build the prediction model using the SparkML library.\n\nThis notebook walks you through these steps:\n\n- Load and Visualize data set.\n- Build a predictive model with SparkML API\n- Save the model in the ML repository"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.0 Install required packages\n\nThere are a couple of Python packages we will use in this notebook.\n\nWML Client: http://ibm-wml-api-pyclient.mybluemix.net/"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "### 1.1 Package Installation"}, {"metadata": {}, "cell_type": "code", "source": "import warnings\nwarnings.filterwarnings('ignore')", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "!pip uninstall watson-machine-learning-client -y | tail -n 1\n!pip uninstall watson-machine-learning-client-V4 -y | tail -n 1\n\n!pip install --upgrade ibm-watson-machine-learning==1.0.38 --user --no-cache | tail -n 1\n!pip install --upgrade pyspark==2.4.0 --user --no-cache | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2 Package Imports"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\nimport pandas as pd\nimport numpy as np\nimport json\nimport os", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2.0 Load and Clean data\n\nWe'll load our data as a pandas data frame.\n\n**<font color='red'><< FOLLOW THE INSTRUCTIONS BELOW TO LOAD THE DATASET >></font>**\n\n* Highlight the cell below by clicking it.\n* Click the `01/00` \"Find data\" icon in the upper right of the notebook.\n* Add the locally uploaded file `german_credit_data.csv` by choosing the `Files` tab. Then choose the `german_credit_data.csv`. Click `Insert to code` and choose `pandas DataFrame`.\n* The code to bring the data into the notebook environment and create a Pandas DataFrame will be added to the cell below.\n* Run the cell\n"}, {"metadata": {}, "cell_type": "code", "source": "# Place cursor below and insert the Pandas DataFrame for the Credit Risk data\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We'll use the Pandas naming convention df for our DataFrame. Make sure that the cell below uses the name for the dataframe used above. For the locally uploaded file it should look like df_data_1 or df_data_2 or df_data_x.\n\n**<font color='red'><< UPDATE THE VARIABLE ASSIGNMENT TO THE VARIABLE GENERATED ABOVE. >></font>**"}, {"metadata": {}, "cell_type": "code", "source": "# Replace df_data_1 with the variable name generated above.\ndf = df_data_1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1 Drop Some Features\nSome columns are data attributes that we will not want to use in the machine learning model. We can drop those columns / features:\n\n- CustomerID feature (column)\n- Personal Attributes: first_name,last_name,email,street_address,city,state,postal_code "}, {"metadata": {}, "cell_type": "code", "source": "#Drop some columns, ignoring errors for missing keys in case we use different data sets.\ndf = df.drop(columns=['CustomerID', 'FirstName', 'LastName', 'Email', 'StreetAddress', 'City', 'State', 'PostalCode', '_ID'], axis=1, errors='ignore')\ndf.head(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2 Examine the data types of the features"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "df.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Statistics for the columns (features). Set it to all, since default is to describe just the numeric features.\ndf.describe(include = 'all')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We see that the loan amounts range from 250 to ~11,600. That the age range for applicants is between 19 and 74. etc."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3 Check for missing data\n\nWe should check if there are missing values in our dataset. There are various ways we can address this issue:\n\n- Drop records with missing values \n- Fill in the missing value with one of the following strategies: Zero, Mean of the values for the column, Random value, etc)."}, {"metadata": {}, "cell_type": "code", "source": "# Check if we have any NaN values and see which features have missing values that should be addressed\nprint(df.isnull().values.any())\ndf.isnull().sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "If there are any missing values from the output above, the sample below would be one approach to handle this issue by imputing the values for the column that reported missing data (i.e the `CurrentResidenceDuration` column in the code as an example):\n"}, {"metadata": {}, "cell_type": "code", "source": "## Handle missing values for nan_column (CurrentResidenceDuration)\n#from sklearn.impute import SimpleImputer\n\n## Find the column number for TotalCharges (starting at 0).\n#target_idx = df.columns.get_loc(\"CurrentResidenceDuration\")\n#imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n#df.iloc[:, target_idx] = imputer.fit_transform(df.iloc[:, target_idx].values.reshape(-1, 1))\n#df.iloc[:, target_idx] = pd.Series(df.iloc[:, target_idx])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.4 Categorize Features\n\nWe will categorize some of the columns / features based on whether they are categorical values or continuous (i.e numerical) values. We will use this in later sections to build visualizations."}, {"metadata": {}, "cell_type": "code", "source": "TARGET_LABEL_COLUMN_NAME = 'Risk'\ncolumns_idx = np.s_[0:] # Slice of first row(header) with all columns.\nfirst_record_idx = np.s_[0] # Index of first record\n\nstring_fields = [type(fld) is str for fld in df.iloc[first_record_idx, columns_idx]] # All string fields\nall_features = [x for x in df.columns if x != TARGET_LABEL_COLUMN_NAME]\ncategorical_columns = list(np.array(df.columns)[columns_idx][string_fields])\ncategorical_features = [x for x in categorical_columns if x != TARGET_LABEL_COLUMN_NAME]\ncontinuous_features = [x for x in all_features if x not in categorical_features]\n\nprint('All Features: ', all_features)\nprint('\\nCategorical Features: ', categorical_features)\nprint('\\nContinuous Features: ', continuous_features)\nprint('\\nAll Categorical Columns: ', categorical_columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.5 Visualize data\n\nData visualization can be used to find patterns, detect outliers, understand distribution and more. We can use graphs such as:\n\n- Histograms, boxplots, etc: To find distribution / spread of our continuous variables.\n- Bar charts: To show frequency in categorical values.\n"}, {"metadata": {}, "cell_type": "code", "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\nsns.set_palette(\"hls\", 3)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "First, we get a high level view of the distribution of Risk. What percentage of applicants in our dataset represent Risk vs No Risk."}, {"metadata": {}, "cell_type": "code", "source": "print(df.groupby([TARGET_LABEL_COLUMN_NAME]).size())\nrisk_plot = sns.countplot(data=df, x=TARGET_LABEL_COLUMN_NAME, order=df[TARGET_LABEL_COLUMN_NAME].value_counts().index)\nplt.ylabel('Count')\nfor p in risk_plot.patches:\n    height = p.get_height()\n    risk_plot.text(p.get_x()+p.get_width()/2., height + 1,'{0:.0%}'.format(height/float(len(df))),ha=\"center\") \nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can get use frequency counts charts to get an understanding of the categorical features relative to Risk\n\n- We can see in the `CheckingStatus` visualization, loan applications with 'no_checking' have a higher occurence of Risk versus loans with other checking status values.\n- We can see in the `CreditHistory` visualization, the loans that have no credits (i.e. all credit has been paid back) have no occurences of Risk (at least in this dataset). There is a small count of Risk for those applicants that have paid back all credit to date. And there is a higher frequency or ratio of Risk for applicants that have existing credit (i.e outstanding credit).\n\n### NOTE: The creation of these plots can take several minutes"}, {"metadata": {}, "cell_type": "code", "source": "# Categorical feature count plots\nf, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10), (ax11, ax12), (ax13, ax14)) = plt.subplots(7, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14 ]\n\nfor i in range(len(categorical_features)):\n    sns.countplot(x = categorical_features[i], hue=TARGET_LABEL_COLUMN_NAME, data=df, ax=ax[i])\n    ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can use histogram and boxplots to get an understanding of the distribution of our continuous / numerical features relative to Risk.\n\n- We can see that for loans that have Risk, the `InstallmentPercent` tends to be higher (i.e. the loans with Risk tend to have loan amounts with higher percentage of the loan applicants disposable income).\n- We can see that those with 'No Risk' seem to be those with fewer existing credit loans at the bank (`ExistingCreditCount`)\n"}, {"metadata": {}, "cell_type": "code", "source": "# Continuous feature histograms.\nf, ((ax1, ax2),(ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]\nfor i in range(len(continuous_features)):\n    #sns.distplot(df[continuous_features[i]], bins=20, color=\"blue\", hist=True, ax=ax[i])\n    sns.distplot(df[df.Risk == 'Risk'][continuous_features[i]], bins=20, color=\"Red\", hist=True, ax=ax[i])\n    sns.distplot(df[df.Risk == 'No Risk'][continuous_features[i]], bins=20, color=\"blue\", hist=True, ax=ax[i])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Plot boxplots of numerical columns. More variation in the boxplot implies higher significance. \nf, ((ax1, ax2),(ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8]\nfor i in range(len(continuous_features)):\n    sns.boxplot(x = TARGET_LABEL_COLUMN_NAME, y = continuous_features[i], data=df, ax=ax[i])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3.0 Create a model\n\nNow we can create our machine learning model. You could use the insights / intuition gained from the data visualization steps above to decide what kind of model to create or which features to use. We will create a simple classification model."}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nimport pandas as pd\nimport json\n\nspark = SparkSession.builder.getOrCreate()\ndf_data = spark.createDataFrame(df)\ndf_data.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1 Split the data into training and test sets"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "spark_df = df_data\n(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\n\nprint(\"Number of records for training: \" + str(train_data.count()))\nprint(\"Number of records for evaluation: \" + str(test_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2 Examine the Spark DataFrame Schema\nLook at the data types to determine requirements for feature engineering"}, {"metadata": {}, "cell_type": "code", "source": "spark_df.printSchema()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.3 Use StringIndexer to encode a string column of labels to a column of label indices\n\nWe are using the Pipeline package to build the development steps as pipeline. \nWe are using StringIndexer to handle categorical / string features from the dataset. StringIndexer encodes a string column of labels to a column of label indices\n\nWe then use VectorAssembler to asemble these features into a vector. Pipelines API requires that input variables are passed in  a vector"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, IndexToString, VectorAssembler, SQLTransformer\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model\n\n#Create StringIndexer columns whose names are same as the categorical column with an appended _IX.\ncategorical_num_features = [x + '_IX' for x in categorical_features]\nsi_list = [StringIndexer(inputCol=nm_in, outputCol=nm_out) for nm_in, nm_out in zip(categorical_features, categorical_num_features)]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Encode our target label column (i.e Risk or No Risk). \n# Also, creates an label convert which performs an inverse map to get back a 'Risk' or 'No Risk' label from the encoded prediction.\nsi_label = StringIndexer(inputCol=TARGET_LABEL_COLUMN_NAME, outputCol=\"label\").fit(spark_df)\nlabel_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_label.labels)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Construct all encoded categorical features plus continuous features into a vector\nva_features = VectorAssembler(inputCols=categorical_num_features + continuous_features, outputCol=\"features\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.4 Create a pipeline, and fit a model using RandomForestClassifier \nAssemble all the stages into a pipeline. We don't expect a clean linear regression, so we'll use RandomForestClassifier to find the best decision tree for the data.\n\nThe pipeline will consist of: the feature string indexing step, the label string indexing step, vector assembly of all features step, random forest classifier, label converter step, and ending with a feature filter step.\n\n**Note: If you want filter features from model output, you could use the feature filter by replacing `*` with feature names to be retained in SQLTransformer statement.**"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "classifier = RandomForestClassifier(featuresCol=\"features\")\nfeature_filter = SQLTransformer(statement=\"SELECT * FROM __THIS__\")\npipeline = Pipeline(stages= si_list + [si_label, va_features, classifier, label_converter, feature_filter])\n\nmodel = pipeline.fit(train_data)", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "predictions = model.transform(test_data)\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderROC')\narea_under_curve = evaluatorDT.evaluate(predictions)\n\nevaluatorDT = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",  metricName='areaUnderPR')\narea_under_PR = evaluatorDT.evaluate(predictions)\n#default evaluation is areaUnderROC\nprint(\"areaUnderROC = %g\" % area_under_curve, \"areaUnderPR = %g\" % area_under_PR)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.5 evaluate more metrics by exporting them into pandas and numpy"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import classification_report\ny_pred = predictions.toPandas()['prediction']\ny_pred = ['Risk' if pred == 1.0 else 'No Risk' for pred in y_pred]\ny_test = test_data.toPandas()[TARGET_LABEL_COLUMN_NAME]\nprint(classification_report(y_test, y_pred, target_names=['Risk', 'No Risk']))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4.0 Save the model\n\nNow the model can be saved for future deployment. The model will be saved using the Watson Machine Learning client, to a deployment space."}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1 Connection to WML\n\nTo authenticate the Watson Machine Learning service on IBM Cloud, you need to provide a platform `api_key` and an endpoint URL. Where the endpoint URL is based on the `location` of the WML instance. To get these values you can use either the IBM Cloud CLI or the IBM Cloud UI.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "#### IBM Cloud CLI\n\nYou can use the [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) to create a platform API Key and retrieve your instance location.\n\n- To generate the Cloud API Key, run the following commands:\n```\nibmcloud login\nibmcloud iam api-key-create API_KEY_NAME\n```\n  - Copy the value of `api_key` from the output.\n\n\n- To retrieve the location of your WML instance, run the following commands:\n```\nibmcloud login --apikey API_KEY -a https://cloud.ibm.com\nibmcloud resource service-instance \"WML_INSTANCE_NAME\"\n```\n> Note: WML_INSTANCE_NAME is the name of your Watson Machine Learning instance and should be quoted in the command.\n\n  - Copy the value of `Location` from the output."}, {"metadata": {}, "cell_type": "markdown", "source": "#### IBM Cloud UI\n\nTo generate Cloud API key:\n- Go to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). \n- From that page, click your name in the top right corner, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. \n- Give your key a name and click **Create**, then copy the created key and to use it below.\n\nTo retrieve the location of your WML instance:\n- Go to the [**Resources List** section of the Cloud console](https://cloud.ibm.com/resources).\n- From that page, expand the **Services** section and find your Watson Machine Learning Instance.\n- Based on the Location displayed in that page, select one of the following values for location variable:\n|Displayed Location|Location|\n|-|-|\n|Dallas|us-south|\n|London|eu-gb|\n|Frankfurt|eu-de|\n|Tokyo|jp-tok|\n"}, {"metadata": {}, "cell_type": "markdown", "source": "**<font color='red'><< Enter your `api_key` and `location` in the following cell. >></font>**"}, {"metadata": {}, "cell_type": "code", "source": "api_key = '<api-key>'\nlocation = '<location>'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_credentials = {\n    \"apikey\": api_key,\n    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_client = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_client.spaces.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.2 Use the desired space as the `default_space`\n\n**<font color='red'><< UPDATE THE VARIABLE 'MODEL_NAME' TO A UNIQUE NAME>></font>**\n\n**<font color='red'><< UPDATE THE VARIABLE 'DEPLOYMENT_SPACE_NAME' TO THE NAME OF THE DEPLOYMENT SPACE CREATED PREVIOUSLY>></font>**\n\nYou should copy the name of your deployment space from the output of the previous cell to the variable in the next cell. The deployment space ID will be looked up based on the name specified below. If you do not receive a space GUID as an output to the next cell, do not proceed until you have created a deployment space."}, {"metadata": {}, "cell_type": "code", "source": "MODEL_NAME = \"<model-name>\"\nDEPLOYMENT_SPACE_NAME = \"<deployment-space-name>\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_client.spaces.list()\nall_spaces = wml_client.spaces.get_details()['resources']\nspace_id = None\nfor space in all_spaces:\n    if space['entity']['name'] == DEPLOYMENT_SPACE_NAME:\n        space_id = space[\"metadata\"][\"id\"]\n        print(\"\\nDeployment Space ID: \", space_id)\n\nif space_id is None:\n    print(\"WARNING: Your space does not exist. Create a deployment space before proceeding to the next cell.\")\n    #space_id = client.spaces.store(meta_props={client.spaces.ConfigurationMetaNames.NAME: space_name})[\"metadata\"][\"guid\"]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Now set the default space to the ID for your deployment space. If this is successful, you will see a 'SUCCESS' message.\nwml_client.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.3 (Optional) Remove Existing Model and Deployment"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "#models = wml_client.repository.get_model_details()\n#model_uid = None\n#for model_in in models['resources']:\n#    if MODEL_NAME == model_in['metadata']['name']:\n#        model_uid = model_in['metadata']['id']\n#        print('Deleting model id', model_uid)\n#        wml_client.repository.delete(model_uid)\n#        break\n#\n#wml_client.repository.list_models()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.4 Save the Model"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "software_spec_uid = wml_client.software_specifications.get_id_by_name(\"spark-mllib_2.4\")\nprint(\"Software Specification ID: {}\".format(software_spec_uid))\n\nmetadata = {\n    wml_client.repository.ModelMetaNames.NAME: MODEL_NAME,\n    wml_client._models.ConfigurationMetaNames.SPACE_UID: space_id,\n    wml_client.repository.ModelMetaNames.TYPE: 'mllib_2.4',\n    wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid\n}\n\npublished_model_details = wml_client.repository.store_model(model, metadata, training_data=df_data,  pipeline=pipeline)\nmodel_uid = wml_client.repository.get_model_uid(published_model_details)\n\nprint(json.dumps(published_model_details, indent=3))\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_client.repository.list_models()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Congratulations, you have created and saved a machine learning model !"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}